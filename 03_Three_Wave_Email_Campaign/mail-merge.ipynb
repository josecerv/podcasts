{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 66042 podcasts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing podcasts: 100%|██████████| 66042/66042 [00:00<00:00, 84583.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test row added with email 'josecerv@wharton.upenn.edu'\n",
      "Test row added with email 'csclark@princeton.edu'\n",
      "Test row added with email 'mohsen.mosleh@gmail.com'\n",
      "Test row added with email 'kmilkman@wharton.upenn.edu'\n",
      "CSV written successfully to podcast_mail_merge_final.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# New mapping dictionaries:\n",
    "lookback_mapping = {\n",
    "    \"1-Year in review\": \"one year\",\n",
    "    \"6-Month lookback\": \"6 month\",\n",
    "    \"Quarter in review\": \"3 month\",\n",
    "    \"2-Month lookback\": \"2 month\",\n",
    "    \"1-Month in review\": \"1 month\"\n",
    "}\n",
    "\n",
    "interval_mapping = {\n",
    "    \"1-Year in review\": \"year\",\n",
    "    \"6-Month lookback\": \"6 months\",\n",
    "    \"Quarter in review\": \"3 months\",\n",
    "    \"2-Month lookback\": \"2 months\",\n",
    "    \"1-Month in review\": \"1 month\"\n",
    "}\n",
    "\n",
    "def safe_json_loads(line):\n",
    "    \"\"\"Safely load JSON, returning None if there's an error.\"\"\"\n",
    "    try:\n",
    "        return json.loads(line.strip())\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Warning: Could not parse JSON line: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_guest_lookup(guest_jsonl_file):\n",
    "    \"\"\"\n",
    "    Creates a lookup dictionary where keys are episode_ids and values are guest counts.\n",
    "    \"\"\"\n",
    "    guest_lookup = {}\n",
    "    try:\n",
    "        with open(guest_jsonl_file, \"r\", encoding=\"utf-8\") as gf:\n",
    "            for line_num, line in enumerate(gf, 1):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    record = json.loads(line)\n",
    "                    episode_id = record.get(\"episode_id\")\n",
    "                    guest_val = int(record.get(\"guests\", 0))\n",
    "                    if episode_id is not None:\n",
    "                        guest_lookup[episode_id] = guest_val\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error processing guest line {line_num}: {e}\")\n",
    "                    continue\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Guest file {guest_jsonl_file} not found\")\n",
    "    return guest_lookup\n",
    "\n",
    "def parse_duration(duration_str):\n",
    "    \"\"\"Converts a duration string (HH:MM:SS, MM:SS, or a raw number) into a number of seconds.\"\"\"\n",
    "    if not duration_str:\n",
    "        return 0\n",
    "    duration_str = str(duration_str).strip()\n",
    "    if \":\" in duration_str:\n",
    "        parts = duration_str.split(\":\")\n",
    "        if len(parts) == 3:\n",
    "            h, m, s = parts\n",
    "            try:\n",
    "                return int(h) * 3600 + int(m) * 60 + int(s)\n",
    "            except ValueError:\n",
    "                return 0\n",
    "        elif len(parts) == 2:\n",
    "            m, s = parts\n",
    "            try:\n",
    "                return int(m) * 60 + int(s)\n",
    "            except ValueError:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        try:\n",
    "            return int(duration_str)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "def process_mail_merge(main_jsonl, guest_jsonl, output_csv):\n",
    "    \"\"\"\n",
    "    Reads the main JSONL file for podcasts and aggregates guest counts from the guest JSONL file.\n",
    "    \"\"\"\n",
    "    # Create guest lookup dictionary\n",
    "    guest_lookup = create_guest_lookup(guest_jsonl)\n",
    "    \n",
    "    try:\n",
    "        with open(main_jsonl, \"r\", encoding=\"utf-8\") as infile, \\\n",
    "             open(output_csv, \"w\", newline='', encoding=\"utf-8\") as outfile:\n",
    "            \n",
    "            csvwriter = csv.writer(outfile)\n",
    "            csvwriter.writerow([\"podcastID\", \"email\", \"podcastName\", \"num_episodes\", \n",
    "                              \"num_guests\", \"duration\", \"interval\", \"lookback\"])\n",
    "            \n",
    "            # Read all podcast records from the JSONL file\n",
    "            podcasts = []\n",
    "            for line_num, line in enumerate(infile, 1):\n",
    "                podcast = safe_json_loads(line)\n",
    "                if podcast is not None:\n",
    "                    podcasts.append(podcast)\n",
    "                else:\n",
    "                    print(f\"Warning: Skipping invalid JSON at line {line_num}\")\n",
    "            \n",
    "            print(f\"Processing {len(podcasts)} podcasts...\")\n",
    "            \n",
    "            last_row = None\n",
    "            for podcast in tqdm(podcasts, desc=\"Processing podcasts\"):\n",
    "                podcastID = podcast.get(\"podcast_id\", \"\")\n",
    "                email = podcast.get(\"email\", \"\")\n",
    "                podcastName = podcast.get(\"title\", \"\")\n",
    "                num_episodes = podcast.get(\"num_episodes\", 0)\n",
    "                original_interval = podcast.get(\"review_label\", \"\")\n",
    "                \n",
    "                interval = interval_mapping.get(original_interval, original_interval)\n",
    "                lookback = lookback_mapping.get(original_interval, original_interval)\n",
    "                \n",
    "                episodes = podcast.get(\"episodes\", [])\n",
    "                \n",
    "                # Calculate total guests only for episodes in prod_db.jsonl\n",
    "                total_guests = 0\n",
    "                for episode in episodes:\n",
    "                    episode_id = episode.get(\"episode_id\")\n",
    "                    if episode_id in guest_lookup:\n",
    "                        total_guests += guest_lookup[episode_id]\n",
    "                \n",
    "                total_duration_seconds = sum(parse_duration(episode.get(\"duration\", \"0\")) for episode in episodes)\n",
    "                total_hours = total_duration_seconds / 3600.0\n",
    "                duration_text = f\"{total_hours:.2f} hours of audio\"\n",
    "                \n",
    "                row_data = [\n",
    "                    podcastID, email, podcastName, num_episodes, \n",
    "                    total_guests, duration_text, interval, lookback\n",
    "                ]\n",
    "                csvwriter.writerow(row_data)\n",
    "                last_row = row_data\n",
    "            \n",
    "            # Add test rows\n",
    "            if last_row:\n",
    "                test_emails = [\n",
    "                    'josecerv@wharton.upenn.edu',\n",
    "                    'csclark@princeton.edu',\n",
    "                    'mohsen.mosleh@gmail.com',\n",
    "                    'kmilkman@wharton.upenn.edu'\n",
    "                ]\n",
    "                \n",
    "                for test_email in test_emails:\n",
    "                    test_row = last_row.copy()\n",
    "                    test_row[1] = test_email\n",
    "                    csvwriter.writerow(test_row)\n",
    "                    print(f\"Test row added with email '{test_email}'\")\n",
    "        \n",
    "        print(f\"CSV written successfully to {output_csv}\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file {main_jsonl} not found\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing files: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    main_jsonl_file = \"prod_db.jsonl\"\n",
    "    guest_jsonl_file = \"guests-extract.jsonl\"\n",
    "    output_file = \"podcast_mail_merge_final.csv\"\n",
    "    process_mail_merge(main_jsonl_file, guest_jsonl_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3821 emails to exclude from email_one_survey.csv.\n",
      "Found 2010 emails to exclude from email_two_survey.csv.\n",
      "Found 7234 emails to exclude based on distribution status in email_one_distribution.csv.\n",
      "Exclusion counts by status in email_one_distribution.csv:\n",
      "- Survey Partially Finished: 2585\n",
      "- Email Hard Bounce: 1973\n",
      "- Survey Finished: 1101\n",
      "- Email Soft Bounce: 903\n",
      "- Session Expired: 671\n",
      "- Email Failed: 1\n",
      "Found 112 emails to exclude based on distribution status in email_two_distribution.csv.\n",
      "Exclusion counts by status in email_two_distribution.csv:\n",
      "- Email Soft Bounce: 66\n",
      "- Email Hard Bounce: 46\n",
      "Reading unsubscribe data from email_one_unsub.csv...\n",
      "Found 949 unsubscribed emails in email_one_unsub.csv.\n",
      "Reading unsubscribe data from email_two_unsub.csv...\n",
      "Found 1131 unsubscribed emails in email_two_unsub.csv.\n",
      "Found 1791 emails to exclude from unsubscribe files.\n",
      "Reading email responses from email_responses.csv...\n",
      "Reading email responses from email_auto-responses.csv...\n",
      "Found 1892 emails to exclude from email response files.\n",
      "Total unique emails to exclude: 12221\n",
      "Exclusion list written to email_exclusions.txt\n",
      "Processing 65563 podcasts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing podcasts: 100%|██████████| 65563/65563 [00:00<00:00, 104383.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test row added for josecerv@wharton.upenn.edu\n",
      "Test row added for csclark@princeton.edu\n",
      "Test row added for mohsen.mosleh@gmail.com\n",
      "Test row added for kmilkman@wharton.upenn.edu\n",
      "CSV written successfully to final_reminder.csv\n",
      "\n",
      "Summary:\n",
      "Total podcasts processed: 65563\n",
      "Excluded podcasts: 11772\n",
      "Included podcasts (including test emails): 53795\n",
      "Total emails in exclusion list: 12221\n",
      "Percentage of podcasts excluded: 17.96%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Existing mapping dictionaries\n",
    "lookback_mapping = {\n",
    "    \"1-Year in review\": \"one year\",\n",
    "    \"6-Month lookback\": \"6 month\",\n",
    "    \"Quarter in review\": \"3 month\",\n",
    "    \"2-Month lookback\": \"2 month\",\n",
    "    \"1-Month in review\": \"1 month\"\n",
    "}\n",
    "\n",
    "interval_mapping = {\n",
    "    \"1-Year in review\": \"year\",\n",
    "    \"6-Month lookback\": \"6 months\",\n",
    "    \"Quarter in review\": \"3 months\",\n",
    "    \"2-Month lookback\": \"2 months\",\n",
    "    \"1-Month in review\": \"1 month\"\n",
    "}\n",
    "\n",
    "# Statuses to exclude\n",
    "EXCLUDE_STATUSES = [\n",
    "    \"Session Expired\", \n",
    "    \"Survey Started\", \n",
    "    \"Survey Partially Finished\", \n",
    "    \"Survey Finished\", \n",
    "    \"Email Soft Bounce\", \n",
    "    \"Email Hard Bounce\", \n",
    "    \"Email Failed\"\n",
    "]\n",
    "\n",
    "def safe_json_loads(line):\n",
    "    \"\"\"Safely load JSON, returning None if there's an error.\"\"\"\n",
    "    try:\n",
    "        return json.loads(line.strip())\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Warning: Could not parse JSON line: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_guest_lookup(guest_jsonl_file):\n",
    "    \"\"\"Creates a lookup dictionary where keys are episode_ids and values are guest counts.\"\"\"\n",
    "    guest_lookup = {}\n",
    "    try:\n",
    "        with open(guest_jsonl_file, \"r\", encoding=\"utf-8\") as gf:\n",
    "            for line_num, line in enumerate(gf, 1):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    record = json.loads(line)\n",
    "                    episode_id = record.get(\"episode_id\")\n",
    "                    guest_val = int(record.get(\"guests\", 0))\n",
    "                    if episode_id is not None:\n",
    "                        guest_lookup[episode_id] = guest_val\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error processing guest line {line_num}: {e}\")\n",
    "                    continue\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Guest file {guest_jsonl_file} not found\")\n",
    "    return guest_lookup\n",
    "\n",
    "def parse_duration(duration_str):\n",
    "    \"\"\"Converts a duration string (HH:MM:SS, MM:SS, or a raw number) into a number of seconds.\"\"\"\n",
    "    if not duration_str:\n",
    "        return 0\n",
    "    duration_str = str(duration_str).strip()\n",
    "    if \":\" in duration_str:\n",
    "        parts = duration_str.split(\":\")\n",
    "        if len(parts) == 3:\n",
    "            h, m, s = parts\n",
    "            try:\n",
    "                return int(h) * 3600 + int(m) * 60 + int(s)\n",
    "            except ValueError:\n",
    "                return 0\n",
    "        elif len(parts) == 2:\n",
    "            m, s = parts\n",
    "            try:\n",
    "                return int(m) * 60 + int(s)\n",
    "            except ValueError:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        try:\n",
    "            return int(duration_str)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "def get_email_response_exclusions(response_file_paths):\n",
    "    \"\"\"Extracts email addresses from email response CSV files.\"\"\"\n",
    "    excluded_emails = set()\n",
    "    \n",
    "    for file_path in response_file_paths:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Warning: Email response file {file_path} not found.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Reading email responses from {file_path}...\")\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                reader = csv.DictReader(file)\n",
    "                for row in reader:\n",
    "                    if 'From: (Address)' in row:\n",
    "                        email = row['From: (Address)'].lower().strip()\n",
    "                        if email and \"@\" in email:  # Basic email validation\n",
    "                            excluded_emails.add(email)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading email response file {file_path}: {e}\")\n",
    "    \n",
    "    print(f\"Found {len(excluded_emails)} emails to exclude from email response files.\")\n",
    "    return excluded_emails\n",
    "\n",
    "def get_unsubscribed_emails(unsub_files):\n",
    "    \"\"\"Get emails from unsubscribe files where Unsubscribed==1.\"\"\"\n",
    "    excluded_emails = set()\n",
    "    \n",
    "    for file_path in unsub_files:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Warning: Unsubscribe file {file_path} not found.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Reading unsubscribe data from {file_path}...\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            if 'Email' in df.columns and 'Unsubscribed' in df.columns:\n",
    "                # Get emails where Unsubscribed == 1\n",
    "                unsubscribed = df[df['Unsubscribed'] == 1]['Email'].str.lower().str.strip()\n",
    "                excluded_emails.update(unsubscribed)\n",
    "                print(f\"Found {len(unsubscribed)} unsubscribed emails in {file_path}.\")\n",
    "            else:\n",
    "                print(f\"Warning: Required columns (Email, Unsubscribed) not found in {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading unsubscribe file {file_path}: {e}\")\n",
    "    \n",
    "    return excluded_emails\n",
    "\n",
    "def get_excluded_emails():\n",
    "    \"\"\"Get emails to exclude from all sources.\"\"\"\n",
    "    excluded_emails = set()\n",
    "    \n",
    "    # 1. Process survey responses (both surveys)\n",
    "    survey_files = [\"email_one_survey.csv\", \"email_two_survey.csv\"]\n",
    "    for survey_file in survey_files:\n",
    "        if os.path.exists(survey_file):\n",
    "            try:\n",
    "                survey_df = pd.read_csv(survey_file)\n",
    "                if 'podcast_email' in survey_df.columns:\n",
    "                    # Add all valid podcast emails from survey responses to exclusion list\n",
    "                    emails = survey_df['podcast_email'].dropna().str.lower().str.strip()\n",
    "                    excluded_emails.update(emails)\n",
    "                    print(f\"Found {len(emails)} emails to exclude from {survey_file}.\")\n",
    "                else:\n",
    "                    print(f\"Warning: 'podcast_email' column not found in {survey_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading survey file {survey_file}: {e}\")\n",
    "        else:\n",
    "            print(f\"Warning: Survey file {survey_file} not found.\")\n",
    "    \n",
    "    # 2. Process distribution status (both distributions)\n",
    "    distribution_files = [\"email_one_distribution.csv\", \"email_two_distribution.csv\"]\n",
    "    for distribution_file in distribution_files:\n",
    "        if os.path.exists(distribution_file):\n",
    "            try:\n",
    "                distribution_df = pd.read_csv(distribution_file)\n",
    "                if 'Email Address' in distribution_df.columns and 'Status' in distribution_df.columns:\n",
    "                    # Filter for the statuses we want to exclude\n",
    "                    status_mask = distribution_df['Status'].isin(EXCLUDE_STATUSES)\n",
    "                    emails_to_exclude = distribution_df.loc[status_mask, 'Email Address'].str.lower().str.strip()\n",
    "                    excluded_emails.update(emails_to_exclude)\n",
    "                    print(f\"Found {len(emails_to_exclude)} emails to exclude based on distribution status in {distribution_file}.\")\n",
    "                    \n",
    "                    # Print count for each excluded status\n",
    "                    status_counts = distribution_df[status_mask]['Status'].value_counts()\n",
    "                    print(f\"Exclusion counts by status in {distribution_file}:\")\n",
    "                    for status, count in status_counts.items():\n",
    "                        print(f\"- {status}: {count}\")\n",
    "                else:\n",
    "                    print(f\"Warning: Required columns not found in {distribution_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading distribution file {distribution_file}: {e}\")\n",
    "        else:\n",
    "            print(f\"Warning: Distribution file {distribution_file} not found.\")\n",
    "    \n",
    "    # 3. Process unsubscribe files\n",
    "    unsub_files = [\"email_one_unsub.csv\", \"email_two_unsub.csv\"]\n",
    "    unsub_exclusions = get_unsubscribed_emails(unsub_files)\n",
    "    excluded_emails.update(unsub_exclusions)\n",
    "    print(f\"Found {len(unsub_exclusions)} emails to exclude from unsubscribe files.\")\n",
    "    \n",
    "    # 4. Process email response files (consolidated across all emails)\n",
    "    email_response_files = [\"email_responses.csv\", \"email_auto-responses.csv\"]\n",
    "    response_exclusions = get_email_response_exclusions(email_response_files)\n",
    "    excluded_emails.update(response_exclusions)\n",
    "    \n",
    "    print(f\"Total unique emails to exclude: {len(excluded_emails)}\")\n",
    "    return excluded_emails\n",
    "\n",
    "def process_mail_merge_with_exclusions(main_jsonl, guest_jsonl, output_csv, exclude_emails):\n",
    "    \"\"\"Processes mail merge while excluding specified emails.\"\"\"\n",
    "    # Create guest lookup dictionary\n",
    "    guest_lookup = create_guest_lookup(guest_jsonl)\n",
    "    \n",
    "    # Track counts for reporting\n",
    "    total_podcasts = 0\n",
    "    excluded_podcasts = 0\n",
    "    included_podcasts = 0\n",
    "    \n",
    "    try:\n",
    "        with open(main_jsonl, \"r\", encoding=\"utf-8\") as infile, \\\n",
    "             open(output_csv, \"w\", newline='', encoding=\"utf-8\") as outfile:\n",
    "            \n",
    "            csvwriter = csv.writer(outfile)\n",
    "            csvwriter.writerow([\"podcastID\", \"email\", \"podcastName\", \"num_episodes\", \n",
    "                              \"num_guests\", \"duration\", \"interval\", \"lookback\"])\n",
    "            \n",
    "            # Read all podcast records from the JSONL file\n",
    "            podcasts = []\n",
    "            for line_num, line in enumerate(infile, 1):\n",
    "                podcast = safe_json_loads(line)\n",
    "                if podcast is not None:\n",
    "                    podcasts.append(podcast)\n",
    "                else:\n",
    "                    print(f\"Warning: Skipping invalid JSON at line {line_num}\")\n",
    "            \n",
    "            total_podcasts = len(podcasts)\n",
    "            print(f\"Processing {total_podcasts} podcasts...\")\n",
    "            \n",
    "            last_row = None\n",
    "            for podcast in tqdm(podcasts, desc=\"Processing podcasts\"):\n",
    "                podcastID = podcast.get(\"podcast_id\", \"\")\n",
    "                email = podcast.get(\"email\", \"\").lower().strip()\n",
    "                \n",
    "                # Skip this podcast if email is in exclusion list\n",
    "                if email in exclude_emails:\n",
    "                    excluded_podcasts += 1\n",
    "                    continue\n",
    "                    \n",
    "                included_podcasts += 1\n",
    "                podcastName = podcast.get(\"title\", \"\")\n",
    "                num_episodes = podcast.get(\"num_episodes\", 0)\n",
    "                original_interval = podcast.get(\"review_label\", \"\")\n",
    "                \n",
    "                interval = interval_mapping.get(original_interval, original_interval)\n",
    "                lookback = lookback_mapping.get(original_interval, original_interval)\n",
    "                \n",
    "                episodes = podcast.get(\"episodes\", [])\n",
    "                \n",
    "                # Calculate total guests\n",
    "                total_guests = 0\n",
    "                for episode in episodes:\n",
    "                    episode_id = episode.get(\"episode_id\")\n",
    "                    if episode_id in guest_lookup:\n",
    "                        total_guests += guest_lookup[episode_id]\n",
    "                \n",
    "                # Calculate total duration\n",
    "                total_duration_seconds = sum(parse_duration(episode.get(\"duration\", \"0\")) for episode in episodes)\n",
    "                total_hours = total_duration_seconds / 3600.0\n",
    "                duration_text = f\"{total_hours:.2f} hours of audio\"\n",
    "                \n",
    "                row_data = [\n",
    "                    podcastID, email, podcastName, num_episodes, \n",
    "                    total_guests, duration_text, interval, lookback\n",
    "                ]\n",
    "                csvwriter.writerow(row_data)\n",
    "                last_row = row_data\n",
    "            \n",
    "            # Add test rows\n",
    "            if last_row:\n",
    "                test_emails = [\n",
    "                    'josecerv@wharton.upenn.edu',\n",
    "                    'csclark@princeton.edu',\n",
    "                    'mohsen.mosleh@gmail.com',\n",
    "                    'kmilkman@wharton.upenn.edu'\n",
    "                ]\n",
    "                \n",
    "                for test_email in test_emails:\n",
    "                    test_row = last_row.copy()\n",
    "                    test_row[1] = test_email\n",
    "                    csvwriter.writerow(test_row)\n",
    "                    print(f\"Test row added for {test_email}\")\n",
    "                \n",
    "                included_podcasts += len(test_emails)\n",
    "        \n",
    "        print(f\"CSV written successfully to {output_csv}\")\n",
    "        print(f\"\\nSummary:\")\n",
    "        print(f\"Total podcasts processed: {total_podcasts}\")\n",
    "        print(f\"Excluded podcasts: {excluded_podcasts}\")\n",
    "        print(f\"Included podcasts (including test emails): {included_podcasts}\")\n",
    "        print(f\"Total emails in exclusion list: {len(exclude_emails)}\")\n",
    "        \n",
    "        # Calculate percentage excluded\n",
    "        if total_podcasts > 0:\n",
    "            exclude_pct = (excluded_podcasts / total_podcasts) * 100\n",
    "            print(f\"Percentage of podcasts excluded: {exclude_pct:.2f}%\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file {main_jsonl} not found\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing files: {e}\")\n",
    "\n",
    "def write_exclusion_list(exclude_emails, output_file=\"email_exclusions.txt\"):\n",
    "    \"\"\"Writes the exclusion list to a file for reference.\"\"\"\n",
    "    try:\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for email in sorted(exclude_emails):\n",
    "                f.write(f\"{email}\\n\")\n",
    "        print(f\"Exclusion list written to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing exclusion list: {e}\")\n",
    "\n",
    "def main():\n",
    "    # Define input files with updated names\n",
    "    main_jsonl_file = \"prod_db3.jsonl\"\n",
    "    guest_jsonl_file = \"guests-extract-combined-v2.jsonl\"\n",
    "    output_file = \"final_reminder.csv\"\n",
    "    \n",
    "    # Get exclusions from all sources\n",
    "    all_exclusions = get_excluded_emails()\n",
    "    \n",
    "    # Write exclusion list to file for reference\n",
    "    write_exclusion_list(all_exclusions)\n",
    "    \n",
    "    # Process mail merge with exclusions\n",
    "    process_mail_merge_with_exclusions(main_jsonl_file, guest_jsonl_file, output_file, all_exclusions)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
