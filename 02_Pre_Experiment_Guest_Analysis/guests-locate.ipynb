{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport re\nimport pandas as pd\nimport jsonlines\nimport csv\nimport requests\nimport time\nimport pickle\nimport signal\nimport logging\nimport pycountry\nfrom sqlalchemy import create_engine\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\nfrom tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom threading import Lock\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Retrieve environment variables\nGOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\nDATABASE_USER = os.getenv('DB_USER')\nDATABASE_PASSWORD = os.getenv('DB_PASSWORD')\nDATABASE_HOST = os.getenv('DB_HOST', 'localhost')\nDATABASE_PORT = os.getenv('DB_PORT', '5432')\nDATABASE_NAME = os.getenv('DB_NAME', 'podcast_episodes')\n\n# Validate environment variables\nmissing_vars = []\nif not GOOGLE_API_KEY:\n    missing_vars.append('GOOGLE_API_KEY')\nif not DATABASE_USER:\n    missing_vars.append('DB_USER')\nif not DATABASE_PASSWORD:\n    missing_vars.append('DB_PASSWORD')\n\nif missing_vars:\n    logging.error(f\"Missing environment variables: {', '.join(missing_vars)}. \"\n                  \"Please set them before running the script.\")\n    exit(1)\n\n# Construct the database connection string\nDATABASE_CONNECTION_STRING = (\n    f\"postgresql+psycopg2://{DATABASE_USER}:{DATABASE_PASSWORD}\"\n    f\"@{DATABASE_HOST}:{DATABASE_PORT}/{DATABASE_NAME}\"\n)\n\n# Categories to investigate\nCATEGORIES = [\n    'Authors and Writers',\n    'Female Voices',\n    'Underrepresented Minority Voices',\n    'Media Professionals'\n]\nTARGET_GENRES = ['Business', 'Society & Culture', 'News']\nMAX_GUESTS_PER_CATEGORY = 100  # Desired number of guests per category\n\n# Global variables\ncurrent_idx = 0\ncategorized_guests = {category: [] for category in CATEGORIES}\napi_cache = {}\noutput_data = []\ncache_lock = Lock()\n\n# ------------------- Signal Handling for Graceful Interruption -------------------\n\ndef signal_handler(signum, frame):\n    print(\"\\nScript interrupted. Saving progress before exiting...\")\n    save_progress()\n    exit(1)\n\nsignal.signal(signal.SIGINT, signal_handler)\n\n# ------------------- Progress Saving and Loading -------------------\n\ndef save_progress():\n    global current_idx, categorized_guests, api_cache, output_data\n    try:\n        with open('progress.pkl', 'wb') as f:\n            pickle.dump({\n                'current_idx': current_idx,\n                'categorized_guests': categorized_guests,\n                'api_cache': api_cache,\n                'output_data': output_data\n            }, f)\n        # Save output_data to CSV\n        if output_data:\n            output_df = pd.DataFrame(output_data)\n            output_df.to_csv('guest_recommendations.csv', index=False)\n            logging.info(\"Partial 'guest_recommendations.csv' has been saved.\")\n        logging.info(\"Progress saved.\")\n    except Exception as e:\n        logging.error(f\"Error saving progress: {e}\")\n\ndef load_progress():\n    global current_idx, categorized_guests, api_cache, output_data\n    if os.path.exists('progress.pkl'):\n        try:\n            with open('progress.pkl', 'rb') as f:\n                progress = pickle.load(f)\n            logging.info(\"Progress loaded.\")\n            current_idx = progress.get('current_idx', 0)\n            categorized_guests = progress.get('categorized_guests', {cat: [] for cat in CATEGORIES})\n            api_cache = progress.get('api_cache', {})\n            output_data = progress.get('output_data', [])\n        except (EOFError, pickle.UnpicklingError) as e:\n            logging.warning(\"Progress file is corrupted or empty. Starting from scratch.\")\n            os.remove('progress.pkl')\n            current_idx = 0\n            categorized_guests = {cat: [] for cat in CATEGORIES}\n            api_cache = {}\n            output_data = []\n    else:\n        current_idx = 0\n        categorized_guests = {cat: [] for cat in CATEGORIES}\n        api_cache = {}\n        output_data = []\n\n# ------------------- Google Knowledge Graph API Interaction -------------------\n\ndef get_guest_info(name, service):\n    \"\"\"\n    Fetch guest information from Google Knowledge Graph API.\n    Implements caching to avoid redundant API calls.\n    \"\"\"\n    with cache_lock:\n        if name in api_cache:\n            return api_cache[name]\n    \n    try:\n        response = service.entities().search(query=name, limit=1).execute()\n        time.sleep(0.1)  # Small delay to respect rate limits\n        if 'itemListElement' in response and len(response['itemListElement']) > 0:\n            entity = response['itemListElement'][0]['result']\n            description = entity.get('description', '')\n            detailed_description = entity.get('detailedDescription', {}).get('articleBody', '')\n            info = (description, detailed_description)\n            with cache_lock:\n                api_cache[name] = info\n            return info\n    except HttpError as e:\n        if e.resp.status == 429:\n            logging.error(\"Quota exceeded for Google Knowledge Graph API.\")\n            raise e\n        else:\n            logging.error(f\"HTTP Error fetching data for {name}: {e}\")\n    except Exception as e:\n        logging.error(f\"Unexpected error fetching data for {name}: {e}\")\n    return ('', '')\n\ndef categorize_guest(guest, service):\n    \"\"\"\n    Categorize a guest into one or more predefined categories.\n    Utilizes Google Knowledge Graph API for additional information.\n    \"\"\"\n    categories = set()\n\n    # Basic attribute checks\n    if guest.get('gender', '').upper() == 'F':\n        categories.add('Female Voices')\n    if guest.get('African-American', False):\n        categories.add('Underrepresented Minority Voices')\n\n    # Fetch additional info from API\n    description, detailed_description = get_guest_info(guest['guest_name'], service)\n    combined_info = f\"{description} {detailed_description}\".lower()\n\n    # Media Professionals\n    media_keywords = [\n        'journalist', 'reporter', 'correspondent', 'editor', 'media',\n        'broadcaster', 'anchor', 'columnist'\n    ]\n    if any(keyword in combined_info for keyword in media_keywords):\n        categories.add('Media Professionals')\n\n    # Authors and Writers\n    if 'author' in combined_info or 'writer' in combined_info:\n        categories.add('Authors and Writers')\n\n    return categories\n\n# ------------------- Parallel Processing Function -------------------\n\ndef process_guest(guest, service):\n    \"\"\"\n    Worker function to process a single guest.\n    Returns a list of categorized guest dictionaries.\n    \"\"\"\n    try:\n        guest_categories = categorize_guest(guest, service)\n        categorized_entries = []\n        for category in guest_categories:\n            with cache_lock:\n                if len(categorized_guests[category]) >= MAX_GUESTS_PER_CATEGORY:\n                    continue\n                entry = {\n                    'category': category,\n                    'guest_name': guest['guest_name'],\n                    'gender': guest['gender'],\n                    'African-American': guest['African-American'],\n                    'podcast_id': guest['podcast_id'],\n                    'episode_id': guest['episode_id'],\n                    'podcast_title': guest['podcast_title'],\n                    'episode_title': guest['episode_title'],\n                    'episode_description': guest['episode_description']\n                }\n                categorized_guests[category].append(entry)\n                output_data.append(entry)\n        return categorized_entries\n    except Exception as e:\n        logging.error(f\"Error processing guest {guest['guest_name']}: {e}\")\n        return []\n\n# ------------------- Main Function -------------------\n\ndef main():\n    global current_idx, categorized_guests, api_cache, output_data\n\n    # Load progress if any\n    load_progress()\n\n    # Initialize Google Knowledge Graph API service\n    try:\n        service = build('kgsearch', 'v1', developerKey=GOOGLE_API_KEY)\n    except Exception as e:\n        logging.error(f\"Error initializing Google API service: {e}\")\n        return\n\n    # Initialize database engine\n    logging.info(\"Connecting to the PostgreSQL database...\")\n    try:\n        engine = create_engine(DATABASE_CONNECTION_STRING)\n    except Exception as e:\n        logging.error(f\"Error connecting to the database: {e}\")\n        return\n\n    # Define the SQL query to fetch episodes_recent data\n    episodes_query = \"\"\"\n    SELECT podcast_id, episode_id, episode_title, episode_description\n    FROM episodes_recent\n    \"\"\"\n\n    # Load guests data from 'guests-extract.jsonl'\n    logging.info(\"Loading guests data from 'guests-extract.jsonl'...\")\n    guests_data = []\n    try:\n        with open('guests-extract.jsonl', 'r', encoding='utf-8') as f:\n            line_number = 0\n            for line in f:\n                line_number += 1\n                line = line.strip()\n                if not line:\n                    logging.warning(f\"Empty line at line {line_number}. Skipping.\")\n                    continue\n                try:\n                    obj = jsonlines.Reader([line]).read()\n                except jsonlines.InvalidLineError as e:\n                    logging.error(f\"Invalid JSON at line {line_number}: {e}. Skipping.\")\n                    continue\n                except Exception as e:\n                    logging.error(f\"Unexpected error at line {line_number}: {e}. Skipping.\")\n                    continue\n\n                podcast_id = obj.get('podcast_id')\n                episode_id = obj.get('episode_id')\n                guests = obj.get('guests', [])\n                for guest in guests:\n                    name = guest.get('name', '').strip()\n                    if not name or len(name.split()) < 2:\n                        continue  # Skip guests without at least first and last names\n                    guest_record = {\n                        'podcast_id': podcast_id,\n                        'episode_id': episode_id,\n                        'guest_name': name,\n                        'gender': guest.get('gender', ''),\n                        'African-American': guest.get('African-American', False)\n                    }\n                    guests_data.append(guest_record)\n    except FileNotFoundError:\n        logging.error(\"File 'guests-extract.jsonl' not found.\")\n        return\n    except Exception as e:\n        logging.error(f\"Error reading 'guests-extract.jsonl': {e}\")\n        return\n\n    guests_df = pd.DataFrame(guests_data)\n    logging.info(f\"Total guests loaded: {len(guests_df)}\")\n\n    # Merge guests with episodes_recent\n    logging.info(\"Merging guests with episodes_recent data...\")\n    try:\n        episodes_iter = pd.read_sql_query(episodes_query, engine, chunksize=100000)\n        merged_chunks = []\n        for chunk_number, chunk in enumerate(episodes_iter, start=1):\n            merged_chunk = pd.merge(\n                guests_df,\n                chunk,\n                on=['podcast_id', 'episode_id'],\n                how='inner'\n            )\n            merged_chunks.append(merged_chunk)\n            logging.info(f\"Processed chunk {chunk_number}: matched {len(merged_chunk)} guests.\")\n        merged_df = pd.concat(merged_chunks, ignore_index=True)\n    except Exception as e:\n        logging.error(f\"Error merging guests with episodes_recent: {e}\")\n        return\n\n    logging.info(f\"Total guests after merging: {len(merged_df)}\")\n\n    # Load podcast data from 'podcasts_sample.csv'\n    logging.info(\"Loading podcast data from 'podcasts_sample.csv'...\")\n    try:\n        # Replace 'sep' based on your CSV delimiter\n        # For comma-separated values\n        podcasts_df = pd.read_csv(\n            'podcasts_sample.csv',\n            sep=',',  # Adjust the separator based on your file\n            encoding='utf-8',\n            dtype={'podcast_id': str},\n            on_bad_lines='skip',\n            quoting=csv.QUOTE_ALL,  # Adjust quoting based on your data\n            escapechar='\\\\'\n        )\n        logging.info(f\"Columns found in podcast data: {podcasts_df.columns.tolist()}\")\n    except Exception as e:\n        logging.error(f\"Error reading 'podcasts_sample.csv': {e}\")\n        return\n\n    # Rename 'title' to 'podcast_title' for consistency\n    if 'title' in podcasts_df.columns:\n        podcasts_df.rename(columns={'title': 'podcast_title'}, inplace=True)\n    else:\n        logging.error(f\"Missing 'title' column in 'podcasts_sample.csv'\")\n        return\n\n    # Verify that required columns are present\n    required_columns = ['podcast_id', 'podcast_title', 'primary_genre']\n    missing_columns = [col for col in required_columns if col not in podcasts_df.columns]\n    if missing_columns:\n        logging.error(f\"Missing columns in 'podcasts_sample.csv': {missing_columns}\")\n        return\n\n    # Merge with podcast titles and genres\n    logging.info(\"Merging with podcast titles and genres...\")\n    try:\n        merged_df = pd.merge(\n            merged_df,\n            podcasts_df[['podcast_id', 'podcast_title', 'primary_genre']],\n            on='podcast_id',\n            how='inner'\n        )\n        logging.info(\"Merge completed.\")\n    except Exception as e:\n        logging.error(f\"Error merging with 'podcasts_sample.csv': {e}\")\n        return\n\n    logging.info(f\"Total guests after merging with podcasts: {len(merged_df)}\")\n\n    # Filter for target genres\n    logging.info(f\"Filtering for target genres: {TARGET_GENRES}\")\n    merged_df = merged_df[merged_df['primary_genre'].isin(TARGET_GENRES)].copy()\n    logging.info(f\"Total guests after genre filtering: {len(merged_df)}\")\n\n    # Remove duplicate guests based on 'guest_name'\n    merged_df = merged_df.drop_duplicates(subset=['guest_name'])\n    logging.info(f\"Total unique guests after removing duplicates: {len(merged_df)}\")\n\n    # Convert merged_df to list of dictionaries for processing\n    guests_list = merged_df.to_dict(orient='records')\n    total_guests = len(guests_list)\n    logging.info(f\"Starting categorization of {total_guests} guests...\")\n\n    # Initialize ThreadPoolExecutor for parallel processing\n    MAX_WORKERS = 20  # Adjust based on your system's capability and API rate limits\n    categorized_guest_count = {category: len(categorized_guests[category]) for category in CATEGORIES}\n\n    # Function to determine if we've reached desired counts\n    def reached_desired_counts():\n        return all(count >= MAX_GUESTS_PER_CATEGORY for count in categorized_guest_count.values())\n\n    # Initialize ThreadPoolExecutor\n    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n        # Create future to guest mapping\n        future_to_guest = {\n            executor.submit(process_guest, guest, service): guest for guest in guests_list\n        }\n\n        # Iterate over completed futures as they finish\n        for future in tqdm(as_completed(future_to_guest), total=total_guests, desc=\"Categorizing Guests\"):\n            guest = future_to_guest[future]\n            try:\n                categorized_entries = future.result()\n                for entry in categorized_entries:\n                    with cache_lock:\n                        categorized_guest_count[entry['category']] += 1\n                # Check if desired counts reached\n                if reached_desired_counts():\n                    logging.info(\"Desired guest counts per category reached. Stopping categorization.\")\n                    break\n            except Exception as e:\n                logging.error(f\"Error in processing guest {guest['guest_name']}: {e}\")\n\n    # Convert output_data to DataFrame\n    output_df = pd.DataFrame(output_data)\n\n    # Ensure each category has exactly MAX_GUESTS_PER_CATEGORY guests\n    for category in CATEGORIES:\n        current_count = len(output_df[output_df['category'] == category])\n        if current_count < MAX_GUESTS_PER_CATEGORY:\n            logging.warning(f\"Category '{category}' has only {current_count} guests.\")\n        elif current_count > MAX_GUESTS_PER_CATEGORY:\n            # Randomly sample if more than MAX_GUESTS_PER_CATEGORY\n            sampled_df = output_df[output_df['category'] == category].sample(\n                n=MAX_GUESTS_PER_CATEGORY, random_state=42\n            )\n            output_df = output_df[~output_df.index.isin(sampled_df.index)]\n            output_df = pd.concat([output_df, sampled_df], ignore_index=True)\n            logging.info(f\"Category '{category}' trimmed to {MAX_GUESTS_PER_CATEGORY} guests.\")\n\n    # Save to CSV\n    try:\n        output_df.to_csv('guest_recommendations.csv', index=False)\n        logging.info(\"CSV file 'guest_recommendations.csv' has been created.\")\n    except Exception as e:\n        logging.error(f\"Error saving 'guest_recommendations.csv': {e}\")\n\n    # Remove progress file if exists\n    if os.path.exists('progress.pkl'):\n        try:\n            os.remove('progress.pkl')\n            logging.info(\"Progress file removed.\")\n        except Exception as e:\n            logging.error(f\"Error removing progress file: {e}\")\n\n    logging.info(\"Script completed successfully.\")\n\nif __name__ == '__main__':\n    main()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}