{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading podcast metadata...\n",
      "Scanning prod_db.jsonl to identify available podcasts...\n",
      "Processed 1000 lines...\n",
      "Processed 2000 lines...\n",
      "Processed 3000 lines...\n",
      "Processed 4000 lines...\n",
      "Processed 5000 lines...\n",
      "Processed 6000 lines...\n",
      "Processed 7000 lines...\n",
      "Processed 8000 lines...\n",
      "Processed 9000 lines...\n",
      "Processed 10000 lines...\n",
      "Processed 11000 lines...\n",
      "Processed 12000 lines...\n",
      "Processed 13000 lines...\n",
      "Processed 14000 lines...\n",
      "Processed 15000 lines...\n",
      "Processed 16000 lines...\n",
      "Processed 17000 lines...\n",
      "Processed 18000 lines...\n",
      "Processed 19000 lines...\n",
      "Processed 20000 lines...\n",
      "Processed 21000 lines...\n",
      "Processed 22000 lines...\n",
      "Processed 23000 lines...\n",
      "Processed 24000 lines...\n",
      "Processed 25000 lines...\n",
      "Processed 26000 lines...\n",
      "Processed 27000 lines...\n",
      "Processed 28000 lines...\n",
      "Processed 29000 lines...\n",
      "Processed 30000 lines...\n",
      "Processed 31000 lines...\n",
      "Processed 32000 lines...\n",
      "Processed 33000 lines...\n",
      "Processed 34000 lines...\n",
      "Processed 35000 lines...\n",
      "Processed 36000 lines...\n",
      "Processed 37000 lines...\n",
      "Processed 38000 lines...\n",
      "Processed 39000 lines...\n",
      "Processed 40000 lines...\n",
      "Processed 41000 lines...\n",
      "Processed 42000 lines...\n",
      "Processed 43000 lines...\n",
      "Processed 44000 lines...\n",
      "Processed 45000 lines...\n",
      "Processed 46000 lines...\n",
      "Processed 47000 lines...\n",
      "Processed 48000 lines...\n",
      "Processed 49000 lines...\n",
      "Processed 50000 lines...\n",
      "Processed 51000 lines...\n",
      "Processed 52000 lines...\n",
      "Processed 53000 lines...\n",
      "Processed 54000 lines...\n",
      "Processed 55000 lines...\n",
      "Processed 56000 lines...\n",
      "Processed 57000 lines...\n",
      "Processed 58000 lines...\n",
      "Processed 59000 lines...\n",
      "Processed 60000 lines...\n",
      "Processed 61000 lines...\n",
      "Processed 62000 lines...\n",
      "Processed 63000 lines...\n",
      "Processed 64000 lines...\n",
      "Processed 65000 lines...\n",
      "Processed 66000 lines...\n",
      "Found 66042 podcasts with episodes in the JSONL file\n",
      "Targeting genres: Business, Society & Culture, News\n",
      "Found 32256 podcasts in target genres with episodes\n",
      "Selected 100 podcasts total\n",
      "Processing RSS feeds for selected podcasts...\n",
      "Processing 100 RSS feeds (only for selected podcasts)...\n",
      "Processed 10/100 RSS feeds...\n",
      "Processed 20/100 RSS feeds...\n",
      "Processed 30/100 RSS feeds...\n",
      "Processed 40/100 RSS feeds...\n",
      "Processed 50/100 RSS feeds...\n",
      "Processed 60/100 RSS feeds...\n",
      "Processed 70/100 RSS feeds...\n",
      "Processed 80/100 RSS feeds...\n",
      "Processed 90/100 RSS feeds...\n",
      "Processed 100/100 RSS feeds...\n",
      "Successfully extracted descriptions for 100 podcasts\n",
      "\n",
      "Genre distribution in final sample:\n",
      "Business: 52\n",
      "Society & Culture: 33\n",
      "News: 22\n",
      "Education: 20\n",
      "Technology: 13\n",
      "Health & Fitness: 9\n",
      "History: 7\n",
      "Science: 5\n",
      "Arts: 3\n",
      "Religion & Spirituality: 3\n",
      "Comedy: 3\n",
      "Sports: 3\n",
      "Music: 2\n",
      "TV & Film: 2\n",
      "Leisure: 2\n",
      "Government: 2\n",
      "True Crime: 1\n",
      "\n",
      "Podcasts with descriptions: 100 out of 100\n",
      "\n",
      "Saved 100 podcasts to poli_sample.jsonl\n",
      "Podcasts with episodes: 100\n",
      "Podcasts with no episodes: 0\n",
      "Total episodes in the sample: 1184\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import feedparser\n",
    "import hashlib\n",
    "from collections import Counter, defaultdict\n",
    "import time\n",
    "import socket\n",
    "import re\n",
    "from datetime import datetime\n",
    "import concurrent.futures\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('podcast_processing.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Constants\n",
    "TIMEOUT = 10  # seconds for RSS feed requests\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text to be UTF-8 compliant and remove problematic characters.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        text = text.encode('utf-8', 'replace').decode('utf-8', 'replace')\n",
    "        text = re.sub(r'[\\x80-\\x9F]', '', text)\n",
    "        text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "        text = re.sub(r'[\\u2028\\u2029\\u0085\\u000A\\u000B\\u000C\\u000D\\u2028\\u2029]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'<[^>]+>', ' ', text)\n",
    "        text = text.replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')\n",
    "        text = re.sub(r'&#?\\w+;', ' ', text)\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error cleaning text: {e}\")\n",
    "        return ''\n",
    "\n",
    "def process_rss_feed(rss_url, podcast_id):\n",
    "    \"\"\"\n",
    "    Process an RSS feed to extract the podcast description.\n",
    "    Returns a dictionary with podcast_id and podcast_description.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        original_timeout = socket.getdefaulttimeout()\n",
    "        socket.setdefaulttimeout(TIMEOUT)\n",
    "        \n",
    "        try:\n",
    "            feed = feedparser.parse(rss_url)\n",
    "        finally:\n",
    "            socket.setdefaulttimeout(original_timeout)\n",
    "            \n",
    "        if not feed or not hasattr(feed, 'feed'):\n",
    "            return None\n",
    "            \n",
    "        # Extract podcast description from the channel level\n",
    "        description = feed.feed.get('description', '')\n",
    "        subtitle = feed.feed.get('subtitle', '')\n",
    "        summary = feed.feed.get('summary', '')\n",
    "        itunes_summary = feed.feed.get('itunes_summary', '')\n",
    "        \n",
    "        # Combine all possible description fields for robustness\n",
    "        all_descriptions = ' '.join(filter(None, [description, subtitle, summary, itunes_summary]))\n",
    "        clean_description = clean_text(all_descriptions)\n",
    "        \n",
    "        if clean_description:\n",
    "            return {\n",
    "                'podcast_id': podcast_id,\n",
    "                'podcast_description': clean_description\n",
    "            }\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing RSS feed for podcast {podcast_id}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"Loading podcast metadata...\")\n",
    "# Load podcasts metadata\n",
    "podcasts_df = pd.read_csv('podcasts_final_sample.csv', low_memory=False)\n",
    "\n",
    "# Create a map of podcast_id to title and RSS URL for easy lookup\n",
    "podcast_titles = dict(zip(podcasts_df['podcast_id'], podcasts_df['title']))\n",
    "podcast_rss_urls = dict(zip(podcasts_df['podcast_id'], podcasts_df['rss']))\n",
    "\n",
    "# Extract primary genres and create a mapping\n",
    "podcast_genres = {}\n",
    "for idx, row in podcasts_df.iterrows():\n",
    "    podcast_id = row['podcast_id']\n",
    "    if pd.isna(row['primary_genre']):\n",
    "        continue\n",
    "    genres = row['primary_genre'].split(', ')\n",
    "    podcast_genres[podcast_id] = genres\n",
    "\n",
    "# First, scan prod_db.jsonl to find which podcasts actually have episodes\n",
    "print(\"Scanning prod_db.jsonl to identify available podcasts...\")\n",
    "available_podcasts = {}\n",
    "\n",
    "# Read the file line by line in binary mode to handle encoding issues\n",
    "with open('prod_db.jsonl', 'rb') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i % 1000 == 0 and i > 0:\n",
    "            print(f\"Processed {i} lines...\")\n",
    "            \n",
    "        try:\n",
    "            # Decode with Latin-1 which can handle any byte sequence\n",
    "            decoded_line = line.decode('latin-1')\n",
    "            data = json.loads(decoded_line)\n",
    "            \n",
    "            # Check if we have a full podcast entry with episodes\n",
    "            if 'podcast_id' in data and 'episodes' in data and isinstance(data['episodes'], list):\n",
    "                podcast_id = data['podcast_id']\n",
    "                title = data.get('title', podcast_titles.get(podcast_id, \"Unknown\"))\n",
    "                episodes = data.get('episodes', [])\n",
    "                \n",
    "                # Store podcast data if it has episodes\n",
    "                if len(episodes) > 0:\n",
    "                    available_podcasts[podcast_id] = {\n",
    "                        'title': title,\n",
    "                        'episodes': episodes[:50],  # Limit to 50 episodes\n",
    "                    }\n",
    "        except Exception as e:\n",
    "            # Just skip problematic lines\n",
    "            continue\n",
    "\n",
    "print(f\"Found {len(available_podcasts)} podcasts with episodes in the JSONL file\")\n",
    "\n",
    "# Target genres we're interested in\n",
    "target_genres = [\"Business\", \"Society & Culture\", \"News\"]\n",
    "print(f\"Targeting genres: {', '.join(target_genres)}\")\n",
    "\n",
    "# Identify podcasts that are both in our target genres AND have episodes\n",
    "target_genre_podcasts = []\n",
    "for podcast_id, genres in podcast_genres.items():\n",
    "    if podcast_id in available_podcasts:\n",
    "        if any(genre in target_genres for genre in genres):\n",
    "            target_genre_podcasts.append(podcast_id)\n",
    "\n",
    "print(f\"Found {len(target_genre_podcasts)} podcasts in target genres with episodes\")\n",
    "\n",
    "# Sample from target genres with episodes\n",
    "sample_size = min(100, len(target_genre_podcasts))\n",
    "if sample_size > 0:\n",
    "    selected_podcasts = np.random.choice(target_genre_podcasts, size=sample_size, replace=False)\n",
    "else:\n",
    "    selected_podcasts = []\n",
    "\n",
    "# If we need more podcasts to reach 100, add others with episodes\n",
    "if len(selected_podcasts) < 100:\n",
    "    remaining_needed = 100 - len(selected_podcasts)\n",
    "    other_available = [pid for pid in available_podcasts if pid not in selected_podcasts]\n",
    "    \n",
    "    additional_count = min(remaining_needed, len(other_available))\n",
    "    if additional_count > 0:\n",
    "        additional_podcasts = np.random.choice(other_available, size=additional_count, replace=False)\n",
    "        selected_podcasts = np.append(selected_podcasts, additional_podcasts)\n",
    "\n",
    "print(f\"Selected {len(selected_podcasts)} podcasts total\")\n",
    "\n",
    "# Now that we have selected the podcasts, process only their RSS feeds\n",
    "print(\"Processing RSS feeds for selected podcasts...\")\n",
    "podcast_descriptions = {}\n",
    "feeds_to_process = []\n",
    "\n",
    "for podcast_id in selected_podcasts:\n",
    "    rss_url = podcast_rss_urls.get(podcast_id)\n",
    "    if pd.notna(rss_url):\n",
    "        feeds_to_process.append((rss_url, podcast_id))\n",
    "\n",
    "print(f\"Processing {len(feeds_to_process)} RSS feeds (only for selected podcasts)...\")\n",
    "\n",
    "# Process feeds in parallel\n",
    "if feeds_to_process:\n",
    "    num_workers = min(32, len(feeds_to_process))\n",
    "    processed = 0\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        future_to_feed = {\n",
    "            executor.submit(process_rss_feed, url, pid): (url, pid) \n",
    "            for url, pid in feeds_to_process\n",
    "        }\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(future_to_feed):\n",
    "            url, pid = future_to_feed[future]\n",
    "            processed += 1\n",
    "            \n",
    "            if processed % 10 == 0:\n",
    "                print(f\"Processed {processed}/{len(feeds_to_process)} RSS feeds...\")\n",
    "                \n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result and result.get('podcast_description'):\n",
    "                    podcast_descriptions[pid] = result['podcast_description']\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing RSS feed for {pid}: {str(e)}\")\n",
    "    \n",
    "    print(f\"Successfully extracted descriptions for {len(podcast_descriptions)} podcasts\")\n",
    "\n",
    "# Create the final output\n",
    "final_sample = []\n",
    "for podcast_id in selected_podcasts:\n",
    "    if podcast_id in available_podcasts:\n",
    "        podcast_data = {\n",
    "            'podcast_id': podcast_id,\n",
    "            'podcast_title': available_podcasts[podcast_id]['title'],\n",
    "            'podcast_description': podcast_descriptions.get(podcast_id, \"\"),  # Include podcast description\n",
    "            'episodes': available_podcasts[podcast_id]['episodes']\n",
    "        }\n",
    "    else:\n",
    "        # Fallback (shouldn't happen with our approach)\n",
    "        podcast_data = {\n",
    "            'podcast_id': podcast_id,\n",
    "            'podcast_title': podcast_titles.get(podcast_id, \"Unknown\"),\n",
    "            'podcast_description': podcast_descriptions.get(podcast_id, \"\"),  # Include podcast description\n",
    "            'episodes': []\n",
    "        }\n",
    "    \n",
    "    final_sample.append(podcast_data)\n",
    "\n",
    "# Check genre distribution in the final sample\n",
    "sampled_genres = []\n",
    "for podcast_id in selected_podcasts:\n",
    "    if podcast_id in podcast_genres:\n",
    "        sampled_genres.extend(podcast_genres[podcast_id])\n",
    "\n",
    "sampled_genre_counts = Counter(sampled_genres)\n",
    "print(\"\\nGenre distribution in final sample:\")\n",
    "for genre, count in sampled_genre_counts.most_common():\n",
    "    print(f\"{genre}: {count}\")\n",
    "\n",
    "# Count podcasts with descriptions\n",
    "podcasts_with_descriptions = sum(1 for p in final_sample if p['podcast_description'])\n",
    "print(f\"\\nPodcasts with descriptions: {podcasts_with_descriptions} out of {len(final_sample)}\")\n",
    "\n",
    "# Write the final sample to the output file\n",
    "with open('poli_sample.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for podcast_data in final_sample:\n",
    "        f.write(json.dumps(podcast_data, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Statistics about the final sample\n",
    "podcasts_with_episodes = sum(1 for p in final_sample if len(p['episodes']) > 0)\n",
    "total_episodes = sum(len(p['episodes']) for p in final_sample)\n",
    "\n",
    "print(f\"\\nSaved {len(final_sample)} podcasts to poli_sample.jsonl\")\n",
    "print(f\"Podcasts with episodes: {podcasts_with_episodes}\")\n",
    "print(f\"Podcasts with no episodes: {len(final_sample) - podcasts_with_episodes}\")\n",
    "print(f\"Total episodes in the sample: {total_episodes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading podcast metadata from podcasts_final_sample.csv...\n",
      "Loading podcast data from poli_sample.jsonl...\n",
      "Found 100 podcasts in poli_sample.jsonl\n",
      "Successfully created podcast_metadata.csv with 100 rows\n",
      "\n",
      "Summary of extracted data:\n",
      "Podcasts with titles: 100\n",
      "Podcasts with descriptions: 100\n",
      "Podcasts with links: 100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def create_podcast_csv():\n",
    "    print(\"Loading podcast metadata from podcasts_final_sample.csv...\")\n",
    "    podcasts_df = pd.read_csv('podcasts_final_sample.csv', low_memory=False)\n",
    "    \n",
    "    # Create a dictionary mapping podcast_id to website\n",
    "    podcast_websites = dict(zip(podcasts_df['podcast_id'], podcasts_df['website']))\n",
    "    \n",
    "    print(\"Loading podcast data from poli_sample.jsonl...\")\n",
    "    podcasts_data = []\n",
    "    \n",
    "    # Read the JSONL file\n",
    "    with open('poli_sample.jsonl', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            podcasts_data.append(json.loads(line))\n",
    "    \n",
    "    print(f\"Found {len(podcasts_data)} podcasts in poli_sample.jsonl\")\n",
    "    \n",
    "    # Create a list to store the extracted data\n",
    "    csv_data = []\n",
    "    \n",
    "    # Extract the required information for each podcast\n",
    "    for podcast in podcasts_data:\n",
    "        podcast_id = podcast.get('podcast_id', '')\n",
    "        \n",
    "        # The 'name' column will be the same as podcast_id (assuming this is what you meant by 'name')\n",
    "        name = podcast_id\n",
    "        \n",
    "        podcast_title = podcast.get('podcast_title', '')\n",
    "        podcast_description = podcast.get('podcast_description', '')\n",
    "        \n",
    "        # Get the website from the dictionary we created\n",
    "        podcast_link = podcast_websites.get(podcast_id, '')\n",
    "        \n",
    "        # Add the data to our list\n",
    "        csv_data.append({\n",
    "            'name': name,\n",
    "            'podcast_title': podcast_title,\n",
    "            'podcast_description': podcast_description,\n",
    "            'podcast_link': podcast_link\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    output_df = pd.DataFrame(csv_data)\n",
    "    \n",
    "    # Write to CSV\n",
    "    output_filename = 'podcast_metadata.csv'\n",
    "    output_df.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Successfully created {output_filename} with {len(output_df)} rows\")\n",
    "    \n",
    "    # Display a summary of the data\n",
    "    print(\"\\nSummary of extracted data:\")\n",
    "    print(f\"Podcasts with titles: {output_df['podcast_title'].notna().sum()}\")\n",
    "    print(f\"Podcasts with descriptions: {output_df['podcast_description'].notna().sum()}\")\n",
    "    print(f\"Podcasts with links: {output_df['podcast_link'].notna().sum()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_podcast_csv()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
