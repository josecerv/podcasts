{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items: 198\n",
      "Total size: 10.21MB\n",
      "Using existing collection ID: 67224ee2e41b4d34e44b5df2\n",
      "Split data into 20 chunks\n",
      "Uploading chunk 0 (size: 0.33MB)\n",
      "Successfully uploaded chunk 0\n",
      "Uploaded chunk 1/20\n",
      "Bin ID for chunk 0: 673e348bacd3cb34a8abdac2\n",
      "Uploading chunk 1 (size: 0.61MB)\n",
      "Compressed size: 0.17MB\n",
      "Successfully uploaded chunk 1\n",
      "Uploaded chunk 2/20\n",
      "Bin ID for chunk 1: 673e348dacd3cb34a8abdac6\n",
      "Uploading chunk 2 (size: 0.57MB)\n",
      "Compressed size: 0.19MB\n",
      "Successfully uploaded chunk 2\n",
      "Uploaded chunk 3/20\n",
      "Bin ID for chunk 2: 673e3490e41b4d34e4579f0a\n",
      "Uploading chunk 3 (size: 0.41MB)\n",
      "Successfully uploaded chunk 3\n",
      "Uploaded chunk 4/20\n",
      "Bin ID for chunk 3: 673e3492ad19ca34f8cd5c35\n",
      "Uploading chunk 4 (size: 0.38MB)\n",
      "Successfully uploaded chunk 4\n",
      "Uploaded chunk 5/20\n",
      "Bin ID for chunk 4: 673e3495acd3cb34a8abdacc\n",
      "Uploading chunk 5 (size: 0.74MB)\n",
      "Compressed size: 0.21MB\n",
      "Successfully uploaded chunk 5\n",
      "Uploaded chunk 6/20\n",
      "Bin ID for chunk 5: 673e3497e41b4d34e4579f11\n",
      "Uploading chunk 6 (size: 0.74MB)\n",
      "Compressed size: 0.20MB\n",
      "Successfully uploaded chunk 6\n",
      "Uploaded chunk 7/20\n",
      "Bin ID for chunk 6: 673e3499ad19ca34f8cd5c3d\n",
      "Uploading chunk 7 (size: 0.27MB)\n",
      "Successfully uploaded chunk 7\n",
      "Uploaded chunk 8/20\n",
      "Bin ID for chunk 7: 673e349ce41b4d34e4579f13\n",
      "Uploading chunk 8 (size: 0.49MB)\n",
      "Successfully uploaded chunk 8\n",
      "Uploaded chunk 9/20\n",
      "Bin ID for chunk 8: 673e349eacd3cb34a8abdad3\n",
      "Uploading chunk 9 (size: 0.58MB)\n",
      "Compressed size: 0.17MB\n",
      "Successfully uploaded chunk 9\n",
      "Uploaded chunk 10/20\n",
      "Bin ID for chunk 9: 673e34a1ad19ca34f8cd5c42\n",
      "Uploading chunk 10 (size: 0.70MB)\n",
      "Compressed size: 0.19MB\n",
      "Successfully uploaded chunk 10\n",
      "Uploaded chunk 11/20\n",
      "Bin ID for chunk 10: 673e34a3acd3cb34a8abdad7\n",
      "Uploading chunk 11 (size: 0.54MB)\n",
      "Compressed size: 0.15MB\n",
      "Successfully uploaded chunk 11\n",
      "Uploaded chunk 12/20\n",
      "Bin ID for chunk 11: 673e34a5ad19ca34f8cd5c48\n",
      "Uploading chunk 12 (size: 0.34MB)\n",
      "Successfully uploaded chunk 12\n",
      "Uploaded chunk 13/20\n",
      "Bin ID for chunk 12: 673e34a7e41b4d34e4579f18\n",
      "Uploading chunk 13 (size: 0.52MB)\n",
      "Compressed size: 0.14MB\n",
      "Successfully uploaded chunk 13\n",
      "Uploaded chunk 14/20\n",
      "Bin ID for chunk 13: 673e34a9acd3cb34a8abdada\n",
      "Uploading chunk 14 (size: 0.45MB)\n",
      "Successfully uploaded chunk 14\n",
      "Uploaded chunk 15/20\n",
      "Bin ID for chunk 14: 673e34ace41b4d34e4579f1b\n",
      "Uploading chunk 15 (size: 0.37MB)\n",
      "Successfully uploaded chunk 15\n",
      "Uploaded chunk 16/20\n",
      "Bin ID for chunk 15: 673e34afe41b4d34e4579f1d\n",
      "Uploading chunk 16 (size: 0.47MB)\n",
      "Successfully uploaded chunk 16\n",
      "Uploaded chunk 17/20\n",
      "Bin ID for chunk 16: 673e34b0acd3cb34a8abdadd\n",
      "Uploading chunk 17 (size: 0.64MB)\n",
      "Compressed size: 0.19MB\n",
      "Successfully uploaded chunk 17\n",
      "Uploaded chunk 18/20\n",
      "Bin ID for chunk 17: 673e34b2acd3cb34a8abdadf\n",
      "Uploading chunk 18 (size: 0.49MB)\n",
      "Successfully uploaded chunk 18\n",
      "Uploaded chunk 19/20\n",
      "Bin ID for chunk 18: 673e34b5e41b4d34e4579f21\n",
      "Uploading chunk 19 (size: 0.56MB)\n",
      "Compressed size: 0.16MB\n",
      "Successfully uploaded chunk 19\n",
      "Uploaded chunk 20/20\n",
      "Bin ID for chunk 19: 673e34b6acd3cb34a8abdae3\n",
      "\n",
      "Upload Complete!\n",
      "Collection ID: 67224ee2e41b4d34e44b5df2\n",
      "Index Bin ID: 673e34b8e41b4d34e4579f24\n",
      "Chunk Bin IDs: ['673e348bacd3cb34a8abdac2', '673e348dacd3cb34a8abdac6', '673e3490e41b4d34e4579f0a', '673e3492ad19ca34f8cd5c35', '673e3495acd3cb34a8abdacc', '673e3497e41b4d34e4579f11', '673e3499ad19ca34f8cd5c3d', '673e349ce41b4d34e4579f13', '673e349eacd3cb34a8abdad3', '673e34a1ad19ca34f8cd5c42', '673e34a3acd3cb34a8abdad7', '673e34a5ad19ca34f8cd5c48', '673e34a7e41b4d34e4579f18', '673e34a9acd3cb34a8abdada', '673e34ace41b4d34e4579f1b', '673e34afe41b4d34e4579f1d', '673e34b0acd3cb34a8abdadd', '673e34b2acd3cb34a8abdadf', '673e34b5e41b4d34e4579f21', '673e34b6acd3cb34a8abdae3']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import sys\n",
    "import gzip\n",
    "import base64\n",
    "\n",
    "API_KEY = '$2a$10$bhPnwdTOKrnJ6a7RgKY8MeqyL.bjndmAI47QRVXV4snklbkmxK2DK'\n",
    "# Use one of your existing collection IDs\n",
    "COLLECTION_ID = \"67224ee2e41b4d34e44b5df2\"  # You can change this to any of your existing collection IDs\n",
    "\n",
    "# Function to estimate size of JSON data\n",
    "def get_size_mb(obj):\n",
    "    return sys.getsizeof(json.dumps(obj)) / (1024 * 1024)\n",
    "\n",
    "# Load the JSONL data\n",
    "podcasts = []\n",
    "with open('final_podcasts.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            podcasts.append(json.loads(line.strip()))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing line: {e}\")\n",
    "            continue\n",
    "\n",
    "# Calculate smaller chunk size - let's try 10 items per chunk\n",
    "CHUNK_SIZE = 10  # Fixed small chunk size\n",
    "\n",
    "print(f\"Total items: {len(podcasts)}\")\n",
    "print(f\"Total size: {get_size_mb(podcasts):.2f}MB\")\n",
    "\n",
    "def split_into_chunks(data, chunk_size):\n",
    "    return [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "def compress_data(data):\n",
    "    json_str = json.dumps(data)\n",
    "    compressed = gzip.compress(json_str.encode('utf-8'))\n",
    "    return base64.b64encode(compressed).decode('utf-8')\n",
    "\n",
    "def upload_chunk(chunk, chunk_id):\n",
    "    url = 'https://api.jsonbin.io/v3/b'\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'X-Master-Key': API_KEY,\n",
    "        'X-Collection-Id': COLLECTION_ID\n",
    "    }\n",
    "    \n",
    "    # Prepare data with compression\n",
    "    data = {\n",
    "        'chunk_id': chunk_id,\n",
    "        'data': chunk,\n",
    "        'compressed': False\n",
    "    }\n",
    "    \n",
    "    chunk_size_mb = get_size_mb(data)\n",
    "    print(f\"Uploading chunk {chunk_id} (size: {chunk_size_mb:.2f}MB)\")\n",
    "    \n",
    "    # If chunk is too large, try compression\n",
    "    if chunk_size_mb > 0.5:  # If larger than 0.5MB, use compression\n",
    "        data['data'] = compress_data(chunk)\n",
    "        data['compressed'] = True\n",
    "        chunk_size_mb = get_size_mb(data)\n",
    "        print(f\"Compressed size: {chunk_size_mb:.2f}MB\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Upload failed for chunk {chunk_id}\")\n",
    "            print(f\"Status code: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            raise Exception(f\"Failed to upload chunk {chunk_id}\")\n",
    "        \n",
    "        result = response.json()\n",
    "        print(f\"Successfully uploaded chunk {chunk_id}\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during upload: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def create_index(chunks):\n",
    "    index = {}\n",
    "    for chunk_id, chunk in enumerate(chunks):\n",
    "        for podcast in chunk:\n",
    "            index[podcast['podcast_id']] = {\n",
    "                'chunk_id': chunk_id,\n",
    "                'podcast_index': chunk.index(podcast)\n",
    "            }\n",
    "    \n",
    "    url = 'https://api.jsonbin.io/v3/b'\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'X-Master-Key': API_KEY,\n",
    "        'X-Collection-Id': COLLECTION_ID\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        'type': 'index',\n",
    "        'data': index\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error creating index. Status code: {response.status_code}\")\n",
    "        print(f\"Response: {response.text}\")\n",
    "        raise Exception(\"Failed to create index\")\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        print(f\"Using existing collection ID: {COLLECTION_ID}\")\n",
    "\n",
    "        # Split the data into chunks\n",
    "        chunks = split_into_chunks(podcasts, CHUNK_SIZE)\n",
    "        print(f\"Split data into {len(chunks)} chunks\")\n",
    "\n",
    "        # Upload each chunk\n",
    "        chunk_bins = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            try:\n",
    "                result = upload_chunk(chunk, i)\n",
    "                bin_id = result['metadata']['id']\n",
    "                chunk_bins.append(bin_id)\n",
    "                print(f\"Uploaded chunk {i+1}/{len(chunks)}\")\n",
    "                print(f\"Bin ID for chunk {i}: {bin_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading chunk {i}: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "        # Create and upload the index\n",
    "        index_result = create_index(chunks)\n",
    "        index_bin_id = index_result['metadata']['id']\n",
    "\n",
    "        print(\"\\nUpload Complete!\")\n",
    "        print(\"Collection ID:\", COLLECTION_ID)\n",
    "        print(\"Index Bin ID:\", index_bin_id)\n",
    "        print(\"Chunk Bin IDs:\", chunk_bins)\n",
    "\n",
    "        # Save the IDs to a file for reference\n",
    "        with open('podcast_bins.json', 'w') as f:\n",
    "            json.dump({\n",
    "                'collection_id': COLLECTION_ID,\n",
    "                'index_bin_id': index_bin_id,\n",
    "                'chunk_bins': chunk_bins\n",
    "            }, f, indent=2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
